{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tasselled Cap Wetness Summary (Roberts Coeffs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook loads surface reflectance data, masks out terrain shadows and calculates tasselled cap wetness for a small polygon area. The output is netcdf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You will need:\n",
    "- to update the output location for the save files to somewhere you have permissions to access\n",
    "- a shapefile of the area of interest (if this is too big you will run into MemoryErrors)\n",
    "\n",
    "\n",
    "#### Things to improve:\n",
    "- TCI coeffs in process of investigation/confirmation -check with author before interpreting.\n",
    "- Write start and end of epoch dates to outfile description\n",
    "\n",
    "##### code written 011117 Bex Dunn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import some useful modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for writing to error files:\n",
    "from __future__ import print_function\n",
    "#get some libraries\n",
    "import datacube\n",
    "import xarray as xr\n",
    "from datacube.storage import masking\n",
    "#from datacube.storage.masking import mask_to_dict #think this is obsolete\n",
    "import json\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from shapely.geometry import shape\n",
    "import numpy as np #need this for pq fuser\n",
    "\n",
    "#libraries for polygon and polygon mask\n",
    "import fiona\n",
    "import shapely.geometry\n",
    "import rasterio.features\n",
    "import rasterio\n",
    "from datacube.utils import geometry\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage.masking import mask_invalid_data\n",
    "\n",
    "#for writing to netcdf\n",
    "from datacube.storage.storage import write_dataset_to_netcdf\n",
    "#dealing with system commands\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "#####These not needed for raijin::::\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#suppress warnings thrown when using inequalities in numpy (the threshold values!)\n",
    "import warnings\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define the functions we need to load and filter data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nbart(sensor,query,bands_of_interest): \n",
    "    '''loads nbart data for a sensor, masks using pq, then filters out terrain -999s\n",
    "    function written 23-08-2017 based on dc v1.5.1'''  \n",
    "    dataset = []\n",
    "    product_name = '{}_{}_albers'.format(sensor, 'nbart')\n",
    "    print('loading {}'.format(product_name))\n",
    "    ds = dc.load(product=product_name, measurements=bands_of_interest,\n",
    "                 group_by='solar_day', **query)\n",
    "    #grab crs defs from loaded ds if ds exists\n",
    "    if ds:\n",
    "        crs = ds.crs\n",
    "        affine = ds.affine\n",
    "        print('loaded {}'.format(product_name))\n",
    "        mask_product = '{}_{}_albers'.format(sensor, 'pq')\n",
    "        sensor_pq = dc.load(product=mask_product, fuse_func=ga_pq_fuser,\n",
    "                            group_by='solar_day', **query)\n",
    "        if sensor_pq:\n",
    "            print('making mask {}'.format(mask_product))\n",
    "            cloud_free = masking.make_mask(sensor_pq.pixelquality,\n",
    "                                           cloud_acca='no_cloud',\n",
    "                                           cloud_shadow_acca = 'no_cloud_shadow',                           \n",
    "                                           cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "                                           cloud_fmask='no_cloud',\n",
    "                                           blue_saturated = False,\n",
    "                                           green_saturated = False,\n",
    "                                           red_saturated = False,\n",
    "                                           nir_saturated = False,\n",
    "                                           swir1_saturated = False,\n",
    "                                           swir2_saturated = False,\n",
    "                                           contiguous=True)\n",
    "            ds = ds.where(cloud_free)\n",
    "            ds.attrs['crs'] = crs\n",
    "            ds.attrs['affine'] = affine\n",
    "            print('masked {} with {} and filtered terrain'.format(product_name,mask_product))\n",
    "            # nbarT is correctly used to correct terrain by replacing -999.0 with nan\n",
    "            ds=ds.where(ds!=-999.0)\n",
    "        else: \n",
    "            print('did not mask {} with {}'.format(product_name,mask_product))\n",
    "    else:\n",
    "        print ('did not load {}'.format(product_name)) \n",
    "\n",
    "    if len(ds)>0:\n",
    "        return ds\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define some functions to calculate the wetness and the wetness summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating tasselled cap wetness\n",
    "def calc_wetness(sensor_data,sensor):\n",
    "    '''This function performs the tasselled cap transformation, multiplying band data by tasselled cap coefficients to \n",
    "    produce a \"wetness\" \"band\".\n",
    "    sensor_data is surface reflectance data loaded from the datacube\n",
    "    sensor = 'ls5, 'ls7' or 'ls8'\n",
    "    Coefficients are from Crist and Cicone 1984 for ls5 and ls7, and from Baig, Zhang, Shuai & Tong for ls8\n",
    "    function written 23-08-2017 based on dc v1.5.1. updated 06-10-2017 bd'''\n",
    "\n",
    "    wetness_coeff = {'ls5':{'blue':0.5702, 'green': 0.1584, 'red':0.2627, 'nir':-0.3959, 'swir1':-0.0045, 'swir2':-0.6511},\n",
    "                    'ls7':{'blue':0.5702, 'green': 0.1584, 'red':0.2627, 'nir':-0.3959, 'swir1':-0.0045, 'swir2':-0.6511},\n",
    "                    'ls8':{'blue':0.5702, 'green': 0.1584, 'red':0.2627, 'nir':-0.3959, 'swir1':-0.0045, 'swir2':-0.6511}}  \n",
    "    #if there is sensor data for the time period\n",
    "    if sensor_data is not None: \n",
    "         # make a deep copy of the sensor data\n",
    "        wbg = sensor_data.copy(deep=True)\n",
    "        #iterate over the spectral bands\n",
    "        for band_name in sensor_data.data_vars:\n",
    "            #multiply each band by the transform coefficient to get a band-specific value\n",
    "            wetness_band = sensor_data[band_name]*wetness_coeff[sensor][band_name]\n",
    "            #update the existing band data with the TC data\n",
    "            #by making new bands, called wet_green, bright_green etc.\n",
    "            wbg.update({'wet_'+band_name:(['time','y','x'],wetness_band)})\n",
    "            #then drop the original bands\n",
    "            wbg = wbg.drop({band_name})    \n",
    "        #sum the values for each band to get the tcw dim    \n",
    "        wbg['wetness']=wbg.wet_blue+wbg.wet_green+wbg.wet_red+wbg.wet_nir+wbg.wet_swir1+wbg.wet_swir2\n",
    "        bands_to_drop =[]\n",
    "        for new_band in wbg.data_vars:\n",
    "            bands_to_drop.append(new_band)            \n",
    "        bands_to_drop.remove('wetness')    \n",
    "        wbg = wbg.drop(bands_to_drop)\n",
    "        print('calculated wetness for {}'.format(sensor))\n",
    "        return wbg    \n",
    "    else:\n",
    "        print('did not calculate wetness for {}'.format(sensor))\n",
    "        return None            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_wetveg_overthresh(wetness,threshold=-400):\n",
    "    '''Calculate the wetness values where wetness>threshold. Inputs are wetness array and threshold value, \n",
    "    default threshold is -400. Band for wetness>threshold is added to wetness. This is not the count.'''\n",
    "    if wetness is not None:\n",
    "        with warnings.catch_warnings():\n",
    "            #suppress irritating behaviour in xarray.where\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            #water_plus_wetveg is wetness values where wetness>threshold\n",
    "            wetness['water_plus_wetveg'] = wetness.wetness.where(wetness.wetness>threshold)\n",
    "            print('thresholded wetness added to array')\n",
    "            return wetness\n",
    "    else:\n",
    "        print('did not calculate wetness overthreshold' )\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_wets(wetness):\n",
    "    '''count the number of wetness scenes for each pixel,\n",
    "    count the amount of times that water plus wet veg is above the threshold\n",
    "    load both into memory (this assumes you are using dask),\n",
    "    return a dictionary of wet count and threshold count'''\n",
    "    if wetness is not None:\n",
    "        #count the number of wetness scenes for each pixel\n",
    "        wet_count = wetness.wetness.count(dim='time')\n",
    "\n",
    "        #count the amount of times that water plus wet veg is above the threshold\n",
    "        threshold_count= wetness.water_plus_wetveg.count(dim='time')\n",
    "        \n",
    "        #bring both counts into memory\n",
    "        wet_count.load()\n",
    "        threshold_count.load()\n",
    "        \n",
    "        #define dictionary of wet count and threshold count\n",
    "        counts = {'wet count':wet_count, 'threshold count':threshold_count}\n",
    "        print('counted')\n",
    "        return counts\n",
    "    else:\n",
    "        print('did not count' )\n",
    "        return None    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define a function to write out to netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_your_netcdf(data, dataset_name, filename,crs):\n",
    "    '''this function turns an xarray dataarray into a dataset so we can write it to netcdf. It adds on a crs definition\n",
    "    from the original array. data = your xarray dataset, dataset_name is a string describing your variable'''    \n",
    "    #turn array into dataset so we can write the netcdf\n",
    "    dataset= data.to_dataset(name=dataset_name)\n",
    "    #grab our crs attributes to write a spatially-referenced netcdf\n",
    "    dataset.attrs['crs'] = crs\n",
    "    #dataset.dataset_name.attrs['crs'] = crs\n",
    "    try:\n",
    "        write_dataset_to_netcdf(dataset, filename)\n",
    "    except RuntimeError as err:\n",
    "        print(\"RuntimeError: {0}\".format(err))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Mainline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the save location for netcdf outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save netcdf outputs to this folder:\n",
    "#netcdf_output_loc ='/g/data/r78/rjd547/groundwater_activities/Analysis/'\n",
    "netcdf_output_loc ='/g/data/r78/rjd547/groundwater_activities/Analysis/TCW_stats/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set the location of the (small) shapefile to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to work with a polygon input\n",
    "shape_file = ('/g/data/r78/rjd547/groundwater_activities/Analysis/slice.shp')\n",
    "# open all the shapes within the shape file\n",
    "shapes = fiona.open(shape_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose which polygon in the shapefile to use (if there are more than one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polygon index is 0\n"
     ]
    }
   ],
   "source": [
    "#i is the index of the polygon in the shape file we have chosen - change i to choose a different polygon\n",
    "i =0 \n",
    "print('polygon index is '+str(i))\n",
    "if i > len(shapes):\n",
    "    print('index not in the range for the shapefile'+str(i)+' not in '+str(len(shapes)))\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get the coordinate reference system from the shapefile, and get the name of the shape from the shapefile too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy attributes from shapefile and define shape_name\n",
    "geom_crs = geometry.CRS(shapes.crs_wkt)\n",
    "geo = shapes[i]['geometry']\n",
    "geom = geometry.Geometry(geo, crs=geom_crs)\n",
    "geom_bs = shapely.geometry.shape(shapes[i]['geometry'])\n",
    "shape_name = shape_file.split('/')[-1].split('.')[0]+'_'+str(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### give the file a name and check if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #check if the file has already been written:\n",
    "filename = netcdf_output_loc+shape_name+'roberts_coeffs.nc'\n",
    "if os.path.isfile(filename):\n",
    "    print('{} already exists'.format(filename))\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tell the datacube which app to use\n",
    "dc = datacube.Datacube(app='dc-nbar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose the time epoch to query the datacube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE SPATIOTEMPORAL RANGE AND BANDS OF INTEREST\n",
    "#Define temporal range\n",
    "start_of_epoch = '2017-01-01'\n",
    "#start_of_epoch = '2016-01-01'\n",
    "#need a variable here that defines a rolling 'latest observation'\n",
    "end_of_epoch =  '2017-12-31'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### choose the spectral bands of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define wavelengths/bands of interest, remove this kwarg to retrieve all bands\n",
    "bands_of_interest = ['blue',\n",
    "                     'green',\n",
    "                     'red',\n",
    "                     'nir',\n",
    "                     'swir1',\n",
    "                     'swir2'\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set up the query that the datacube will read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'time': (start_of_epoch, end_of_epoch), 'geopolygon': geom\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load in the nbart data for each sensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ls5_nbart_albers\n",
      "did not load ls5_nbart_albers\n",
      "loading ls7_nbart_albers\n",
      "loaded ls7_nbart_albers\n",
      "making mask ls7_pq_albers\n",
      "masked ls7_nbart_albers with ls7_pq_albers and filtered terrain\n",
      "loading ls8_nbart_albers\n"
     ]
    }
   ],
   "source": [
    "#this is done separately instead of in a loop because the datasets can be quite large.\n",
    "#currently this is a way of memory handling -there is probably a better way of doing it.\n",
    "sensor1_nbart=load_nbart('ls5',query,bands_of_interest)\n",
    "sensor2_nbart=load_nbart('ls7',query,bands_of_interest)\n",
    "sensor3_nbart=load_nbart('ls8',query,bands_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate wetness for each timeslice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wetness_sensor1_nbart=calc_wetness(sensor1_nbart,'ls5')\n",
    "wetness_sensor2_nbart=calc_wetness(sensor2_nbart,'ls7')\n",
    "wetness_sensor3_nbart=calc_wetness(sensor3_nbart,'ls8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate wetness over the threshold for each timeslice (remove values under the threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "water_plus_wetveg_1 =calc_wetveg_overthresh(wetness_sensor1_nbart)\n",
    "water_plus_wetveg_2 =calc_wetveg_overthresh(wetness_sensor2_nbart)\n",
    "water_plus_wetveg_3 =calc_wetveg_overthresh(wetness_sensor3_nbart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count number of wetness scenes and number of times tcw above threshold for each pixel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_sensor_1_nbart = count_wets(wetness_sensor1_nbart)\n",
    "counts_sensor_2_nbart = count_wets(wetness_sensor2_nbart)\n",
    "counts_sensor_3_nbart = count_wets(wetness_sensor3_nbart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## divide the number of times wetness is seen per pixel by the number of wetness scenes per pixel to get a proportion of time that the pixel is wet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_list = [counts_sensor_1_nbart, counts_sensor_2_nbart,counts_sensor_3_nbart]\n",
    "threshold_list =[]\n",
    "wet_list=[]\n",
    "for acount in counts_list:\n",
    "    #test for data existence\n",
    "    if acount is not None:\n",
    "        wet_count = acount['wet count']\n",
    "        threshold = acount['threshold count']\n",
    "        threshold_list.append(threshold)\n",
    "        wet_list.append(wet_count)\n",
    "#times wetness is over threshold by pixel         \n",
    "threshold_allsensors = sum(threshold_list) \n",
    "#number of wetness scenes by pixel\n",
    "wet_count_allsensors = sum(wet_list)        \n",
    "wet_proportion_allsensors = threshold_allsensors/wet_count_allsensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the tasselled cap wetness summary for your polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wet_proportion_allsensors.plot(cmap ='gist_earth_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('successfully ran TCW for '+shape_name+' polygon number '+str(i))\n",
    "eprint('successfully ran TCW for '+shape_name+' polygon number '+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the crs for the netcdf from whichever wetness array actually has one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if wetness_sensor1_nbart is not None:\n",
    "    crs = wetness_sensor1_nbart.crs\n",
    "else:\n",
    "    if wetness_sensor2_nbart is not None:\n",
    "        crs = wetness_sensor2_nbart.crs\n",
    "    else: \n",
    "        crs = wetness_sensor3_nbart.crs\n",
    "print(crs)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_your_netcdf(wet_proportion_allsensors,'tcw',filename=filename, crs=crs)\n",
    "print('successfully wrote tcw netcdf for '+shape_name+' polygon number '+str(i))\n",
    "eprint('successfully wrote tcw netcdf for for '+shape_name+' polygon number '+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear_observations is count of wetness scenes at pixel\n",
    "write_your_netcdf(wet_count_allsensors,'clearobs',filename=netcdf_output_loc+shape_name+'_clearobs.nc',crs=crs)\n",
    "print('successfully wrote clearobs netCDF for '+shape_name+' polygon number '+str(i))\n",
    "eprint('successfully wrote clearobs netCDFfor '+shape_name+' polygon number '+str(i))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If the two cells above were successful you now have netcdfs of tasselled cap wetness and a the clear observations for your location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
