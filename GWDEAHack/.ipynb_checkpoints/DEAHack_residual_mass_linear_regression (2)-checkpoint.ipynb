{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demostrates how to load landsat data and bom rainfall grids, calculate NDVI, calculate the mass residual curve from the rainfall data, index the ndvi data by the averaged residual mass curve and run a linear regression on this indexed data. The application is to determine what change in vegetation condition occurs across years as a result of annual rainfall variability.\n",
    "\n",
    "Completed as part of the November 2017 DEA goehack\n",
    "\n",
    "Neil SYmington - neil.symington@ga.gov.au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for writing to error files:\n",
    "from __future__ import print_function\n",
    "#get some libraries\n",
    "import datacube\n",
    "import xarray as xr\n",
    "from datacube.storage import masking\n",
    "#from datacube.storage.masking import mask_to_dict #think this is obsolete\n",
    "import json\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from shapely.geometry import shape\n",
    "import numpy as np #need this for pq fuser\n",
    "\n",
    "#libraries for polygon and polygon mask\n",
    "import fiona\n",
    "import shapely.geometry\n",
    "import rasterio.features\n",
    "import rasterio\n",
    "from datacube.utils import geometry\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage.masking import mask_invalid_data\n",
    "\n",
    "#for writing to netcdf\n",
    "from datacube.storage.storage import write_dataset_to_netcdf\n",
    "#dealing with system commands\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "#####These not needed for raijin::::\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#suppress warnings thrown when using inequalities in numpy (the threshold values!)\n",
    "import warnings\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nbart(sensor,query,bands_of_interest): \n",
    "    '''loads nbart data for a sensor, masks using pq, then filters out terrain -999s\n",
    "    function written 23-08-2017 based on dc v1.5.1'''  \n",
    "    dataset = []\n",
    "    product_name = '{}_{}_albers'.format(sensor, 'nbart')\n",
    "    print('loading {}'.format(product_name))\n",
    "    ds = dc.load(product=product_name, measurements=bands_of_interest,\n",
    "                 group_by='solar_day', **query)\n",
    "    #grab crs defs from loaded ds if ds exists\n",
    "    if ds:\n",
    "        crs = ds.crs\n",
    "        affine = ds.affine\n",
    "        print('loaded {}'.format(product_name))\n",
    "        mask_product = '{}_{}_albers'.format(sensor, 'pq')\n",
    "        sensor_pq = dc.load(product=mask_product, fuse_func=ga_pq_fuser,\n",
    "                            group_by='solar_day', **query)\n",
    "        if sensor_pq:\n",
    "            print('making mask {}'.format(mask_product))\n",
    "            cloud_free = masking.make_mask(sensor_pq.pixelquality,\n",
    "                                           cloud_acca='no_cloud',\n",
    "                                           cloud_shadow_acca = 'no_cloud_shadow',                           \n",
    "                                           cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "                                           cloud_fmask='no_cloud',\n",
    "                                           blue_saturated = False,\n",
    "                                           green_saturated = False,\n",
    "                                           red_saturated = False,\n",
    "                                           nir_saturated = False,\n",
    "                                           swir1_saturated = False,\n",
    "                                           swir2_saturated = False,\n",
    "                                           contiguous=True)\n",
    "            ds = ds.where(cloud_free)\n",
    "            ds.attrs['crs'] = crs\n",
    "            ds.attrs['affine'] = affine\n",
    "            print('masked {} with {} and filtered terrain'.format(product_name,mask_product))\n",
    "            # nbarT is correctly used to correct terrain by replacing -999.0 with nan\n",
    "            ds=ds.where(ds!=-999.0)\n",
    "        else: \n",
    "            print('did not mask {} with {}'.format(product_name,mask_product))\n",
    "    else:\n",
    "        print ('did not load {}'.format(product_name)) \n",
    "\n",
    "    if len(ds)>0:\n",
    "        return ds\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tell the datacube which app to use\n",
    "dc = datacube.Datacube(app='dc-nbar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE SPATIOTEMPORAL RANGE AND BANDS OF INTEREST\n",
    "#Define temporal range\n",
    "start_of_epoch = '1987-01-01'\n",
    "#start_of_epoch = '2016-01-01'\n",
    "#need a variable here that defines a rolling 'latest observation'\n",
    "end_of_epoch =  '2017-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define wavelengths/bands of interest, remove this kwarg to retrieve all bands\n",
    "bands_of_interest = [#'blue',\n",
    "                     #'green',\n",
    "                     'red',\n",
    "                     'nir',\n",
    "                     #'swir1',\n",
    "                     #'swir2'\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'time': (start_of_epoch, end_of_epoch)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-7-bc3069428f9c>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-bc3069428f9c>\"\u001b[0;36m, line \u001b[0;32m5\u001b[0m\n\u001b[0;31m    lon_min = 128.619555°\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "#If not using a polygon/polyline, enter lat/lon here manually\n",
    "lat_max = -14.859923\n",
    "lat_min = -15.307703\n",
    "lon_max = 128.955193\n",
    "lon_min = 128.619555°\n",
    "query['x'] = (lon_min, lon_max)\n",
    "query['y'] = (lat_max, lat_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is done separately instead of in a loop because the datasets can be quite large.\n",
    "#currently this is a way of memory handling -there is probably a better way of doing it.\n",
    "sensor1_nbart=load_nbart('ls5',query,bands_of_interest)\n",
    "sensor2_nbart=load_nbart('ls7',query,bands_of_interest)\n",
    "sensor3_nbart=load_nbart('ls8',query,bands_of_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sensor1_nbart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_sens = xr.concat([sensor1_nbart, sensor2_nbart, sensor3_nbart], 'time')\n",
    "all_sens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sens['ndvi'] = (all_sens['nir'] - all_sens['red']) / (all_sens['nir'] + all_sens['red'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load bom_rainfall grids from the datacube (or from file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup rain directory where we have put our pickle data\n",
    "rainpath ='/g/data1/zk34/njs547/Rainfall_EK'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        os.mkdir(rainpath)\n",
    "except OSError as err:\n",
    "        print(\"OS error: {0}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit query if need be\n",
    "\n",
    "query['time'] = ('1978-01-01', '2017-11-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "try:\n",
    "    #try to get the rain data from a pickle file saved earlier\n",
    "    f = open(rainpath+'/DEA_Hack_rainfall_data_OB'+'.pkl', 'rb')\n",
    "    rain = pickle.load(f)\n",
    "    Studysite_rain = rain['Studysite_rain']\n",
    "    print('loaded rainfall grids from file:'+rainpath+'DEA_Hack_rainfall_data_OB.pkl')\n",
    "    f.close()\n",
    "except:\n",
    "    #Grab bom_rainfall_grids from the datacube\n",
    "    print('loading bom rainfall grids from datacube')\n",
    "    Studysite_rain = dc.load(product = 'bom_rainfall_grids', **query)\n",
    "    #make a dictionary of the data we want to save\n",
    "    vars2pickle = {'Studysite_rain':Studysite_rain}\n",
    "    f = open(rainpath+'/DEA_Hack_rainfall_data_OB.pkl', 'wb')\n",
    "    pickle.dump(vars2pickle,f) \n",
    "    print('saving rainfall data to file')\n",
    "    #pickle.dump(vars2pickle,f,protocol = 2, fix_imports = True) #maintain compatibility with python 2\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "fig = Studysite_rain.rainfall.isel(time = [0]).plot()\n",
    "#reverse the colourmap so high rainfall is blue\n",
    "fig.set_cmap('viridis_r')\n",
    "#print (Studysite_rain_masked)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample xarray Dataset Studysite_rain by year 'AS' \n",
    "month_sum = Studysite_rain.resample('MS', dim='time', how='sum', keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(month_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot\n",
    "\n",
    "month_sum.rainfall.mean(dim = ['longitude', 'latitude']).plot()\n",
    "print(month_sum.time)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the residual mass rainfall curve\n",
    "\n",
    "def calculate_residual_mass_curve(a):\n",
    "    # find the number of time steps (ie. years)\n",
    "    \n",
    "    n = len(a.rainfall.time)/12\n",
    "    \n",
    "    # First calculate a cumulative rainfall xarray from the rainfall data\n",
    "    \n",
    "    arr = a.rainfall.values\n",
    "    \n",
    "    cum_rf = np.cumsum(arr, axis = 0)\n",
    "    \n",
    "    cum_rf_xr = xr.DataArray(cum_rf, dims = ('time', 'latitude', 'longitude'),\n",
    "                              coords = [a.time, a.latitude, a.longitude])\n",
    "    \n",
    "    # NOw we will calculate a cumulative rainfall assuming average rainfall on a month by month basis\n",
    "    # Find the average of all months\n",
    "    ave_months = a.rainfall.groupby('time.month').mean('time').values\n",
    "   \n",
    "    # In the case that we are not starting from January we will need to reorder the array\n",
    "    \n",
    "    start_month = a.time[0].dt.month.values - 1\n",
    "    \n",
    "    ave_month = np.concatenate((ave_months[start_month:,:,:], ave_months[0:start_month,:,:]), axis = 0)\n",
    "\n",
    "    \n",
    "    # Tile an array so that we can run a cumulative sum on it\n",
    "    tiled_ave = np.tile(ave_months, (round(n), 1, 1))\n",
    "    \n",
    "    # In the case that we have residual months remove them from the tiled array\n",
    "    if (n).is_integer() == False:\n",
    "        month_remainder = int(round((n%1) * 12))\n",
    "\n",
    "        tiled_ave = tiled_ave[:int(-month_remainder),:,:]\n",
    "        \n",
    "    # Generate the cumulative sum of rainfall one would get assuming average rainfall every month\n",
    "    cum_ave = np.cumsum(tiled_ave, axis = 0)\n",
    "    \n",
    "    cum_ave_xr = xr.DataArray(cum_ave, dims = ('time', 'latitude', 'longitude'),\n",
    "                              coords = [a.time, a.latitude, a.longitude])\n",
    "    \n",
    "    # The mass residual curve is the difference between the cumulativer rainfall data and the cumulative\n",
    "    # rainfall one would get iff the average always occured\n",
    "    mass_res_curve = cum_rf_xr - cum_ave_xr\n",
    "    \n",
    "    return mass_res_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mass_res_curve = calculate_residual_mass_curve(month_sum)    \n",
    "\n",
    "test_mass_res_curve.mean(dim = ('latitude', 'longitude')).plot()\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOw grounp the mass residual curve by calendar year\n",
    "yearly_mass_res = mass_res_curve.groupby('time.year').mean()\n",
    "\n",
    "yearly_mass_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set name for meging purposes\n",
    "\n",
    "yearly_mass_res.name = 'Averaged mass residual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_mass_res.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we get back to our nbar data and we calculate the 25th percentile NDVI for each year\n",
    "\n",
    "def quantile(x):\n",
    "    return x.quantile(0.25, dim = 'time')\n",
    "\n",
    "\n",
    "ndvi_p25 = all_sens.ndvi.groupby('time.year').apply(quantile)\n",
    "\n",
    "ndvi_p25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the nbar data with the yearly average residual madd\n",
    "\n",
    "merged_data = xr.merge([ndvi_p25, yearly_mass_res])\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop nulls\n",
    "merged_data = merged_data.dropna(dim = 'year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to convert to a dataframe as xarray is not up to date and has no sortby function\n",
    "\n",
    "merged_data_sorted = merged_data.sortby('Averaged mass residual', ascending = False)\n",
    "\n",
    "merged_data_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "#This function applies a linear regression to a grid over a set time interval\n",
    "def linear_regression_grid(input_array, mask_no_trend = True, NDVI = False):\n",
    "    '''\n",
    "    This function applies a linear regression to a grid over a set time interval by looping through lat and lon \n",
    "    and calculating the linear regression through time for each pixel.\n",
    "    '''\n",
    "    print(input_array.year)\n",
    "    ylen = len(input_array.y)\n",
    "    xlen = len(input_array.x)\n",
    "    from itertools import product\n",
    "    coordinates = product(range(ylen), range(xlen))\n",
    "\n",
    "    slopes = np.zeros((ylen, xlen))\n",
    "    p_values = np.zeros((ylen, xlen))\n",
    "    print('Slope shape is ', slopes.shape)\n",
    "\n",
    "    for y, x in coordinates:\n",
    "        val = input_array.isel(x = x, y = y)\n",
    "        # If analysing NDVI data replace negative numbers which are spurious for NDVI with nans\n",
    "        if NDVI == True:\n",
    "            val[val<0] = np.nan\n",
    "\n",
    "            # Check that we have at least three values to perform our linear regression on\n",
    "            if np.count_nonzero(~np.isnan(val)) > 3:\n",
    "                if str(val.dims[0]) == 'month':\n",
    "                    slopes[y, x], intercept, r_sq, p_values[y, x], std_err = stats.linregress(val.month,val)\n",
    "                elif str(val.dims[0]) == 'year':\n",
    "                    slopes[y, x], intercept, r_sq, p_values[y, x], std_err = stats.linregress(val.year,val)\n",
    "            else:\n",
    "                slopes[y, x] = np.nan\n",
    "                intercept = np.nan\n",
    "                r_sq = np.nan\n",
    "                p_values[y, x] = np.nan\n",
    "        else:\n",
    "            if str(val.dims[0]) == 'month':\n",
    "                slopes[y, x], intercept, r_sq, p_values[y, x], std_err = stats.linregress(val.month,val)\n",
    "            elif str(val.dims[0]) == 'year':\n",
    "                slopes[y, x], intercept, r_sq, p_values[y, x], std_err = stats.linregress(val.year,val)\n",
    "\n",
    "    #Get coordinates from the original xarray\n",
    "    lat  = input_array.coords['y']\n",
    "    long = input_array.coords['x']\n",
    "    #Mask out values with insignificant trends (ie. p-value > 0.05) if user wants\n",
    "    if mask_no_trend == True:\n",
    "        slopes[p_values>0.05]=np.nan        \n",
    "    # Write arrays into a x-array\n",
    "    slope_xr = xr.DataArray(slopes, coords = [lat, long], dims = ['y', 'x'])\n",
    "    p_val_xr = xr.DataArray(p_values, coords = [lat, long], dims = ['y', 'x']) \n",
    "    return slope_xr, p_val_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linear regression\n",
    "\n",
    "slope_xr, p_val_xr = linear_regression_grid(merged_data_sorted.ndvi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the grid\n",
    "\n",
    "slope_xr.plot()\n",
    "plt.savefig('/home/547/njs547/DEA_hack.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_val_xr.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
