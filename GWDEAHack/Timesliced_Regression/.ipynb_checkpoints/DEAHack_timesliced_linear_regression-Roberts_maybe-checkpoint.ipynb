{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demostrates how to load landsat data and bom rainfall grids, calculate NDVI, calculate the mass residual curve from the rainfall data, index the ndvi data by the averaged residual mass curve and run a linear regression on this indexed data. The application is to determine what change in vegetation condition occurs across years as a result of annual rainfall variability.\n",
    "\n",
    "Completed as part of the November 2017 DEA goehack\n",
    "\n",
    "Neil SYmington - neil.symington@ga.gov.au"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for writing to error files:\n",
    "from __future__ import print_function\n",
    "#get some libraries\n",
    "import datacube\n",
    "import xarray as xr\n",
    "from datacube.storage import masking\n",
    "#from datacube.storage.masking import mask_to_dict #think this is obsolete\n",
    "import json\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from shapely.geometry import shape\n",
    "import numpy as np #need this for pq fuser\n",
    "from datetime import datetime\n",
    "from scipy import stats\n",
    "#libraries for polygon and polygon mask\n",
    "import fiona\n",
    "import shapely.geometry\n",
    "import rasterio.features\n",
    "import rasterio\n",
    "from datacube.utils import geometry\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage.masking import mask_invalid_data\n",
    "\n",
    "#for writing to netcdf\n",
    "from datacube.storage.storage import write_dataset_to_netcdf\n",
    "#dealing with system commands\n",
    "import sys\n",
    "import os.path\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "#####These not needed for raijin::::\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.dates\n",
    "import matplotlib as mpl\n",
    "\n",
    "#suppress warnings thrown when using inequalities in numpy (the threshold values!)\n",
    "import warnings\n",
    "\n",
    "def eprint(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_nbart(sensor,query,bands_of_interest): \n",
    "    '''loads nbart data for a sensor, masks using pq, then filters out terrain -999s\n",
    "    function written 23-08-2017 based on dc v1.5.1'''  \n",
    "    dataset = []\n",
    "    product_name = '{}_{}_albers'.format(sensor, 'nbart')\n",
    "    print('loading {}'.format(product_name))\n",
    "    ds = dc.load(product=product_name, measurements=bands_of_interest,\n",
    "                 group_by='solar_day', **query)\n",
    "    #grab crs defs from loaded ds if ds exists\n",
    "    if ds:\n",
    "        crs = ds.crs\n",
    "        affine = ds.affine\n",
    "        print('loaded {}'.format(product_name))\n",
    "        mask_product = '{}_{}_albers'.format(sensor, 'pq')\n",
    "        sensor_pq = dc.load(product=mask_product, fuse_func=ga_pq_fuser,\n",
    "                            group_by='solar_day', **query)\n",
    "        if sensor_pq:\n",
    "            print('making mask {}'.format(mask_product))\n",
    "            cloud_free = masking.make_mask(sensor_pq.pixelquality,\n",
    "                                           cloud_acca='no_cloud',\n",
    "                                           cloud_shadow_acca = 'no_cloud_shadow',                           \n",
    "                                           cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "                                           cloud_fmask='no_cloud',\n",
    "                                           blue_saturated = False,\n",
    "                                           green_saturated = False,\n",
    "                                           red_saturated = False,\n",
    "                                           nir_saturated = False,\n",
    "                                           swir1_saturated = False,\n",
    "                                           swir2_saturated = False,\n",
    "                                           contiguous=True)\n",
    "            ds = ds.where(cloud_free)\n",
    "            ds.attrs['crs'] = crs\n",
    "            ds.attrs['affine'] = affine\n",
    "            print('masked {} with {} and filtered terrain'.format(product_name,mask_product))\n",
    "            # nbarT is correctly used to correct terrain by replacing -999.0 with nan\n",
    "            ds=ds.where(ds!=-999.0)\n",
    "        else: \n",
    "            print('did not mask {} with {}'.format(product_name,mask_product))\n",
    "    else:\n",
    "        print ('did not load {}'.format(product_name)) \n",
    "\n",
    "    if len(ds)>0:\n",
    "        return ds\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculating tasselled cap wetness for all sensors\n",
    "def calc_wetness_DR2017(sensor_data):\n",
    "    '''This function performs the tasselled cap transformation, multiplying band data by tasselled cap coefficients to \n",
    "    produce a \"wetness\" \"band\".\n",
    "    sensor_data is surface reflectance data loaded from the datacube\n",
    "    Coefficients are from Roberts 2018 in press\n",
    "    function written 23-08-2017 based on dc v1.5.1. updated 19-10-2017 bd updated 16-11-17 to use one set of coefficients\n",
    "    and not specify which sensor as the coefficients are based on the processed nbart'''\n",
    "\n",
    "    wetness_coeff = {'blue':0.5702, 'green': 0.1584, 'red':0.2627, 'nir':-0.3959, 'swir1':-0.0045, 'swir2':-0.6511}\n",
    "                    \n",
    "    if sensor_data is not None: \n",
    "         # make a deep copy of the sensor data\n",
    "        wbg = sensor_data.copy(deep=True)\n",
    "        #iterate over the spectral bands\n",
    "        for band_name in sensor_data.data_vars:\n",
    "            #multiply each band by the transform coefficient to get a band-specific value\n",
    "            wetness_band = sensor_data[band_name]*wetness_coeff[band_name]\n",
    "            #update the existing band data with the TC data\n",
    "            #by making new bands, called wet_green, bright_green etc.\n",
    "            wbg.update({'wet_'+band_name:(['time','y','x'],wetness_band)})\n",
    "            #then drop the original bands\n",
    "            wbg = wbg.drop({band_name})    \n",
    "        #sum the values for each band to get the tcw dim    \n",
    "        wbg['wetness']=wbg.wet_blue+wbg.wet_green+wbg.wet_red+wbg.wet_nir+wbg.wet_swir1+wbg.wet_swir2\n",
    "        bands_to_drop =[]\n",
    "        for new_band in wbg.data_vars:\n",
    "            bands_to_drop.append(new_band)            \n",
    "        bands_to_drop.remove('wetness')    \n",
    "        wbg = wbg.drop(bands_to_drop)\n",
    "        print('calculated wetness')\n",
    "        return wbg    \n",
    "    else:\n",
    "        print('did not calculate wetness')\n",
    "        return None            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tell the datacube which app to use\n",
    "dc = datacube.Datacube(app='dc-nbar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### DEFINE SPATIOTEMPORAL RANGE AND BANDS OF INTEREST\n",
    "#Define temporal range\n",
    "start_of_epoch = '1987-01-01'\n",
    "#start_of_epoch = '2016-01-01'\n",
    "#need a variable here that defines a rolling 'latest observation'\n",
    "end_of_epoch =  '2017-12-31'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define wavelengths/bands of interest, remove this kwarg to retrieve all bands\n",
    "bands_of_interest = ['blue',\n",
    "                     'green',\n",
    "                     'red',\n",
    "                     'nir',\n",
    "                     'swir1',\n",
    "                     'swir2'\n",
    "                     ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OB_springs_0\n"
     ]
    }
   ],
   "source": [
    "#shape_file = ('/home/547/njs547/shapefiles/OB_springs.shp')\n",
    "shape_file = ('/home/547/rjd547/jupyter_notebooks/GWandDEA_bex_ness/GWDEAHack/Timesliced_Regression/From_Neil_TR/OB_springs.shp')\n",
    "# open all the shapes within the shape file\n",
    "shapes = fiona.open(shape_file)\n",
    "#choose the index of the polygon you want within the shape file\n",
    "i =0\n",
    "#copy attributes from shapefile and define shape_name\n",
    "geom_crs = geometry.CRS(shapes.crs_wkt)\n",
    "geo = shapes[i]['geometry']\n",
    "geom = geometry.Geometry(geo, crs=geom_crs)\n",
    "geom_bs = shapely.geometry.shape(shapes[i]['geometry'])\n",
    "shape_name = shape_file.split('/')[-1].split('.')[0]+'_'+str(i)\n",
    "print(shape_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = {\n",
    "    'time': (start_of_epoch, end_of_epoch), 'geopolygon': geom,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If not using a polygon/polyline, enter lat/lon here manually\n",
    "#lat_max = -14.886385\n",
    "#lat_min = -14.921916\n",
    "#lon_max = 128.670053\n",
    "#lon_min = 128.62809\n",
    "#query['x'] = (lon_min, lon_max)\n",
    "#query['y'] = (lat_max, lat_min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ls5_nbart_albers\n"
     ]
    }
   ],
   "source": [
    "#this is done separately instead of in a loop because the datasets can be quite large.\n",
    "#currently this is a way of memory handling -there is probably a better way of doing it.\n",
    "sensor1_nbart=load_nbart('ls5',query,bands_of_interest)\n",
    "sensor2_nbart=load_nbart('ls7',query,bands_of_interest)\n",
    "sensor3_nbart=load_nbart('ls8',query,bands_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### concatenate sensor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make an array of all the clean sensor data\n",
    "sensor_list = []\n",
    "for sensor in [sensor1_nbart, sensor2_nbart, sensor3_nbart]:\n",
    "    if sensor is not None:\n",
    "        sensor_list.append(sensor)\n",
    "nbart_allsensors = xr.concat(sensor_list,dim='time')\n",
    "#steal affine for use in transforming image coords later on\n",
    "affine = nbart_allsensors.affine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del sensor1_nbart\n",
    "del sensor2_nbart\n",
    "del sensor3_nbart\n",
    "del sensor_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sort by time so that sensors are interleaved properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbart_allsensors =nbart_allsensors.sortby('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### calculate TC wetness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbart_allsensors_wetness = calc_wetness_DR2017(nbart_allsensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load bom_rainfall grids from the datacube (or from file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup rain directory where we have put our pickle data\n",
    "rainpath ='/home/547/rjd547/jupyter_notebooks/GWandDEA_bex_ness/GWDEAHack/Timesliced_Regression/From_Neil_TR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "        os.mkdir(rainpath)\n",
    "except OSError as err:\n",
    "        print(\"OS error: {0}\".format(err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Edit query if need be\n",
    "\n",
    "# query['time'] = ('1978-01-01', '2017-11-01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "try:\n",
    "    #try to get the rain data from a pickle file saved earlier\n",
    "    f = open(rainpath+'/DEA_Hack_rainfall_data_L.pkl', 'rb')\n",
    "    rain = pickle.load(f)\n",
    "    Studysite_rain = rain['Studysite_rain']\n",
    "    print('loaded rainfall grids from file:'+rainpath+'DEA_Hack_rainfall_data.pkl')\n",
    "    f.close()\n",
    "except:\n",
    "    #Grab bom_rainfall_grids from the datacube\n",
    "    print('loading bom rainfall grids from datacube')\n",
    "    Studysite_rain = dc.load(product = 'bom_rainfall_grids', **query)\n",
    "    #make a dictionary of the data we want to save\n",
    "    vars2pickle = {'Studysite_rain':Studysite_rain}\n",
    "    f = open(rainpath+'/DEA_Hack_rainfall_data_L.pkl', 'wb')\n",
    "    pickle.dump(vars2pickle,f) \n",
    "    print('saving rainfall data to file')\n",
    "    #pickle.dump(vars2pickle,f,protocol = 2, fix_imports = True) #maintain compatibility with python 2\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(4,3))\n",
    "fig = Studysite_rain.rainfall.isel(time = [0]).plot()\n",
    "#reverse the colourmap so high rainfall is blue\n",
    "fig.set_cmap('viridis_r')\n",
    "#print (Studysite_rain_masked)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resample xarray Dataset Studysite_rain by year 'AS' \n",
    "month_sum = Studysite_rain.resample('MS', dim='time', how='sum', keep_attrs=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(month_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLot\n",
    "\n",
    "month_sum.rainfall.mean(dim = ['longitude', 'latitude']).plot()\n",
    "print(month_sum.time)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the residual mass rainfall curve\n",
    "\n",
    "def calculate_residual_mass_curve(a):\n",
    "    # find the number of time steps (ie. years)\n",
    "    \n",
    "    n = len(a.rainfall.time)/12\n",
    "    \n",
    "    # First calculate a cumulative rainfall xarray from the rainfall data\n",
    "    \n",
    "    arr = a.rainfall.values\n",
    "    \n",
    "    cum_rf = np.cumsum(arr, axis = 0)\n",
    "    \n",
    "    cum_rf_xr = xr.DataArray(cum_rf, dims = ('time', 'latitude', 'longitude'),\n",
    "                            coords = [a.time, a.latitude, a.longitude])\n",
    "    \n",
    "    # NOw we will calculate a cumulative rainfall assuming average rainfall on a month by month basis\n",
    "    # Find the average of all months\n",
    "    ave_months = a.rainfall.groupby('time.month').mean('time').values\n",
    "   \n",
    "    # In the case that we are not starting from January we will need to reorder the array\n",
    "    \n",
    "    start_month = a.time[0].dt.month.values - 1\n",
    "    \n",
    "    ave_month = np.concatenate((ave_months[start_month:,:,:], ave_months[0:start_month,:,:]), axis = 0)\n",
    "\n",
    "    \n",
    "    # Tile an array so that we can run a cumulative sum on it\n",
    "    tiled_ave = np.tile(ave_months, (round(n), 1, 1))\n",
    "    \n",
    "    # In the case that we have residual months remove them from the tiled array\n",
    "    if (n).is_integer() == False:\n",
    "        month_remainder = int(round((n%1) * 12))\n",
    "\n",
    "        tiled_ave = tiled_ave[:int(-month_remainder),:,:]\n",
    "        \n",
    "    # Generate the cumulative sum of rainfall one would get assuming average rainfall every month\n",
    "    cum_ave = np.cumsum(tiled_ave, axis = 0)\n",
    "    \n",
    "    cum_ave_xr = xr.DataArray(cum_ave, dims = ('time', 'latitude', 'longitude'),\n",
    "                              coords = [a.time, a.latitude, a.longitude])\n",
    "    \n",
    "    # The mass residual curve is the difference between the cumulativer rainfall data and the cumulative\n",
    "    # rainfall one would get iff the average always occured\n",
    "    mass_res_curve = cum_rf_xr - cum_ave_xr\n",
    "    \n",
    "    return mass_res_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mass_res_curve = calculate_residual_mass_curve(month_sum)    \n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOw grounp the mass residual curve by calendar year\n",
    "yearly_mass_res = mass_res_curve.groupby('time.year').mean()\n",
    "\n",
    "yearly_mass_res\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set name for meging purposes\n",
    "\n",
    "yearly_mass_res.name = 'Averaged mass residual'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yearly_mass_res.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now slice the data array between the years of interest\n",
    "\n",
    "sliced_yearly_mass_res = yearly_mass_res.loc[dict(year = slice(start_of_epoch.split('-')[0], end_of_epoch.split('-')[0]))]\n",
    "len(sliced_yearly_mass_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOw we want to split the residual mass into trending periods. For this we will define a function for extracting\n",
    "# linear regression slopes on variable window sizes with various start and end points\n",
    "\n",
    "def extract_trend(mass_res, min_length = 4):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for i in range(min_length, len(mass_res) + 1):\n",
    "        for j in range(0, len(mass_res) - (i - 1)):\n",
    "\n",
    "            arr = mass_res.values[j: j + i]\n",
    "\n",
    "            start_yr = mass_res[j].year.values\n",
    "            end_yr = mass_res[j + i - 1].year.values\n",
    "            slope, intercept, r, p_value, std_err = stats.linregress(np.arange(0,len(arr)), arr)\n",
    "            \n",
    "            results.append([start_yr, end_yr, slope, p_value, r**2])\n",
    "            \n",
    "\n",
    "    return np.array(results)\n",
    "\n",
    "\n",
    "a = extract_trend(sliced_yearly_mass_res, 5)                        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the greatest positive trend. For this we need to weight up multiple criteria. We want a strong trend (ie. large\n",
    "# slope coefficient) as well as a long time period and statistical significance (ie. p_value < 0.05)\n",
    "\n",
    "# To examine this a bit more we plot \n",
    "\n",
    "\n",
    "# Keep only positive a\n",
    "\n",
    "pos_a = a[a[:,2] > 0]\n",
    "\n",
    "nyears = pos_a[:,1] - pos_a[:,0]\n",
    "slopes = pos_a[:,2]\n",
    "r_squ = pos_a[:,4]\n",
    "\n",
    "plt.scatter(x = nyears, y = slopes, c = r_squ, cmap=cm.jet)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This problem really becomes one of weighing up criteria. For the sake of this we will produce a 'function' for decision\n",
    "# making\n",
    "\n",
    "def scale(a):\n",
    "    scaled = np.array([np.float(i)/np.sum(a) for i in a])\n",
    "\n",
    "    return scaled\n",
    "\n",
    "def pick_best_range(slope, nyears, r_squ):\n",
    "    \n",
    "    # first scale the data so the magnitude matters less\n",
    "    \n",
    "    slope_sc = scale(slope)\n",
    "    nyears_sc = scale(nyears)\n",
    "    r_squ_sc = scale(r_squ)\n",
    "    \n",
    "    # define coefficients\n",
    "    a = 0.7\n",
    "    b = 1\n",
    "    c = 1\n",
    "    \n",
    "    # multiply the dataests by these numbers\n",
    "    \n",
    "    slope_wtd = a * slope_sc\n",
    "    nyears_wtd = b * nyears_sc\n",
    "    r_squ_wtd = c * r_squ_sc\n",
    "    \n",
    "    # Sum the criteria\n",
    "    \n",
    "    score = np.sum([slope_wtd, nyears_wtd, r_squ_wtd], axis = 0)\n",
    "    \n",
    "    \n",
    "    # Find the maximum of the summed array\n",
    "    \n",
    "    idx = np.argmax(score)\n",
    "    \n",
    "    return idx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = pick_best_range(slopes, nyears, r_squ)\n",
    "pos_year_range = pos_a[best_idx,0:2]\n",
    "print(pos_year_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do the same for the negative trend\n",
    "\n",
    "\n",
    "# Keep only negative a\n",
    "neg_a = a[a[:,2] < 0]\n",
    "\n",
    "nyears = neg_a[:,1] - neg_a[:,0]\n",
    "slopes = neg_a[:,2]\n",
    "r_squ = neg_a[:,4]\n",
    "\n",
    "plt.scatter(x = nyears, y = slopes, c = r_squ, cmap=cm.jet)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_idx = pick_best_range(-1 * slopes, nyears, r_squ)\n",
    "neg_year_range = neg_a[best_idx,0:2]\n",
    "print(neg_year_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we get back to our nbar data and we calculate the 25th percentile TCW for each year\n",
    "\n",
    "def quantile(x):\n",
    "    return x.quantile(0.25, dim = 'time')\n",
    "\n",
    "\n",
    "tcw_p25 = nbart_allsensors_wetness.groupby('time.year').apply(quantile)\n",
    "\n",
    "tcw_p25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Now we get back to our nbar data and we calculate the 25th percentile NDVI for each year\n",
    "\n",
    "# def quantile(x):\n",
    "#     return x.quantile(0.25, dim = 'time')\n",
    "\n",
    "\n",
    "# ndvi_p25 = all_sens.ndvi.groupby('time.year').apply(quantile)\n",
    "\n",
    "# ndvi_p25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "\n",
    "#This function applies a linear regression to a grid over a set time interval\n",
    "def linear_regression_grid(input_array, mask_no_trend = False, NDVI = False):\n",
    "    '''\n",
    "    This function applies a linear regression to a grid over a set time interval by looping through lat and lon \n",
    "    and calculating the linear regression through time for each pixel.\n",
    "    '''\n",
    "    print(input_array.year)\n",
    "    ylen = len(input_array.y)\n",
    "    xlen = len(input_array.x)\n",
    "    from itertools import product\n",
    "    coordinates = product(range(ylen), range(xlen))\n",
    "\n",
    "    slopes = np.zeros((ylen, xlen))\n",
    "    p_values = np.zeros((ylen, xlen))\n",
    "    print('Slope shape is ', slopes.shape)\n",
    "\n",
    "    for y, x in coordinates:\n",
    "        val = input_array.isel(x = x, y = y)\n",
    "        # If analysing NDVI data replace negative numbers which are spurious for NDVI with nans\n",
    "        if NDVI == True:\n",
    "            val[val<0] = np.nan\n",
    "\n",
    "            # Check that we have at least three values to perform our linear regression on\n",
    "            if np.count_nonzero(~np.isnan(val)) > 3:\n",
    "                if str(val.dims[0]) == 'month':\n",
    "                    slopes[y, x], intercept, r_sq, p_values[y, x], std_err = stats.linregress(val.month,val)\n",
    "                elif str(val.dims[0]) == 'year':\n",
    "                    slopes[y, x], intercept, r_sq, p_values[y, x], std_err = stats.linregress(val.year,val)\n",
    "            else:\n",
    "                slopes[y, x] = np.nan\n",
    "                intercept = np.nan\n",
    "                r_sq = np.nan\n",
    "                p_values[y, x] = np.nan\n",
    "        else:\n",
    "            if str(val.dims[0]) == 'month':\n",
    "                slopes[y, x], intercept, r_sq, p_values[y, x], std_err = stats.linregress(val.month,val)\n",
    "            elif str(val.dims[0]) == 'year':\n",
    "                slopes[y, x], intercept, r_sq, p_values[y, x], std_err = stats.linregress(val.year,val)\n",
    "\n",
    "    #Get coordinates from the original xarray\n",
    "    lat  = input_array.coords['y']\n",
    "    long = input_array.coords['x']\n",
    "    #Mask out values with insignificant trends (ie. p-value > 0.05) if user wants\n",
    "    if mask_no_trend == True:\n",
    "        slopes[p_values>0.05]=np.nan        \n",
    "    # Write arrays into a x-array\n",
    "    slope_xr = xr.DataArray(slopes, coords = [lat, long], dims = ['y', 'x'])\n",
    "    p_val_xr = xr.DataArray(p_values, coords = [lat, long], dims = ['y', 'x']) \n",
    "    return slope_xr, p_val_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tcw_p25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice using the positive and negative mass residual rainfall trends desccribed above\n",
    "\n",
    "pos_rmc_slope_tcw = tcw_p25.wetness.sel(year = slice(pos_year_range[0],pos_year_range[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_rmc_slope_tcw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slice using the positive and negative mass residual rainfall trends desccribed above\n",
    "\n",
    "neg_rmc_slope_tcw = tcw_p25.wetness.sel(year = slice(neg_year_range[0],neg_year_range[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_rmc_slope_tcw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linear regression\n",
    "\n",
    "slope_xr, p_val_xr = linear_regression_grid(pos_rmc_slope_tcw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the grid\n",
    "\n",
    "slope_xr.plot()\n",
    "#plt.savefig('/home/547/njs547/DEA_hack.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_val_xr.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run linear regression\n",
    "\n",
    "slope_xr_n, p_val_xr_n = linear_regression_grid(neg_rmc_slope_tcw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the grid\n",
    "\n",
    "slope_xr_n.plot()\n",
    "#plt.savefig('/home/547/njs547/DEA_hack.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val_xr_n.plot()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove scenes with more than 80% nan values  to remove scenes that are mostly cloud for Hov plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pernan is a filtration level - scenes with more nans than this per scene are removed\n",
    "#multiply area by 6 to get number of bands\n",
    "pernan = 0.99\n",
    "nbart_allsensors_wetness_nanfiltered = nbart_allsensors_wetness['wetness'].dropna('time',  thresh = int(pernan*len(nbart_allsensors_wetness.x)*len(nbart_allsensors_wetness.y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " MAss residual curve idexed Hovmoller plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is used to plot the hovmoller plot. HEre we assign a y index (ie. \n",
    "#index of constant y coordinate) but the function could be adapted to take in shapefiles.\n",
    "\n",
    "def plot_hov(sliced_all_sens, y_index = 300):\n",
    "       \n",
    "    \n",
    "    #Define colour strecthes    \n",
    "#     tcw_cmap = mpl.colors.ListedColormap(['darkslategray','dimgray' , 'lightgrey' , '#ccff66' , '#2eb82e', '#009933'])\n",
    "\n",
    "#     tcw_bounds = [-1, 0, 0.1, 0.2, 0.3, 0.5, 0.8, 1]\n",
    "\n",
    "#     tcw_norm = mpl.colors.BoundaryNorm(tcw_bounds, tcw_cmap.N)\n",
    "\n",
    "    # Plot\n",
    "\n",
    "    fig = plt.figure(figsize=(11.69,8.27))\n",
    "\n",
    "    # Write an new dataset using \n",
    "    sliced_all_sens.isel(y = y_index).plot(cmap ='viridis', yincrease = False)\n",
    "\n",
    "    ax = plt.gca()\n",
    "\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose the y index\n",
    "plt.imshow(slope_xr)\n",
    "plt.plot([0,305], [105, 105], 'r')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function is a hacky slice using years\n",
    "\n",
    "def hack_slice(ds, years):\n",
    "    # Yearly data will be appended to a list\n",
    "    year_chunks = []\n",
    "    \n",
    "    # Iterate through input years\n",
    "    for item in years:\n",
    "\n",
    "        year = str(item)\n",
    "\n",
    "        t = ds.time.dt.year\n",
    "        \n",
    "        # If the years exist in the satellite data\n",
    "        if len(t.loc[year]) != 0:\n",
    "            # This is a very hacky slice using a mask because regular slicing was not possible due to a bug in xarray\n",
    "            mask = t.where(t == int(year), drop = True)\n",
    "            sliced_ds = ds.loc[dict(time = mask.time)]\n",
    "            # Append the results to a list\n",
    "            year_chunks.append(sliced_ds)\n",
    "            \n",
    "    # Finall concatenate all of the data into a new dataset\n",
    "\n",
    "    sliced_ds = xr.concat(year_chunks, dim = 'time')\n",
    "    return sliced_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot Hovmoller for the positive trend\n",
    "\n",
    "years = np.arange(int(pos_year_range[0]), int(pos_year_range[1]), 1)\n",
    "\n",
    "years = [int(x) for x in years]\n",
    "\n",
    "print(years)\n",
    "\n",
    "pos_rmc_slope_all_sens = hack_slice(nbart_allsensors_wetness_nanfiltered, years)\n",
    "#print(pos_rmc_slope_all_sens.time)\n",
    "\n",
    "plot_hov(pos_rmc_slope_all_sens, y_index = 105)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Hovmoller for the negative trend\n",
    "\n",
    "years = np.arange(int(neg_year_range[0]), int(neg_year_range[1]), 1)\n",
    "\n",
    "years = [int(x) for x in years]\n",
    "\n",
    "print(years)\n",
    "\n",
    "neg_rmc_slope_all_sens = hack_slice(all_sens, years)\n",
    "print(neg_rmc_slope_all_sens.time)\n",
    "\n",
    "plot_hov(neg_rmc_slope_all_sens, y_index = 105)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
