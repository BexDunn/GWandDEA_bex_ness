{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make a geomedian composite, plot an RGB, save a netCDF and GeoTiff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook takes data from one landsat sensor, plots it and then saves the output to netcdf and to geotiff!\n",
    "\n",
    "Make sure you have run the bash script (by running source jload_stats.sh) on the command line before you open this notebook,\n",
    "otherwise it won't work.\n",
    "\n",
    "This notebook is setup to use an input polygon, but you can specify lat/long if you prefer.\n",
    "You also need to update the input and output paths so you are saving to your own directories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T04:31:37.844175Z",
     "start_time": "2018-02-14T04:31:37.839592Z"
    }
   },
   "source": [
    "Note about landsat data availability:\n",
    "\n",
    "- Landsat 5 - 1986 to April 1999 followed by a gap until May 2003 - November 2011 (data from 2009 onwards becomes less reliable in southern Australia)\n",
    "- Landsat 7 - April 1999 to present, however after May 2003 the scan line corrector (SLC) failed, so data are referred to as SLC-off, meaning they've got a venetian blinds appearance with wedges of missing data. This data is not well suited for inclusion in composites, but is fine to use in time series analysis\n",
    "- Landsat 8 - April 2013 onwards\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook created by Bex Dunn, Vanessa Newey, Josh Sixsmith and Erin Telfer, February 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T00:09:12.648359Z",
     "start_time": "2018-02-26T00:09:12.436114Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### check that we have the correct modules loaded into your terminal session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T05:49:33.078005Z",
     "start_time": "2018-02-26T05:49:32.899569Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently Loaded Modulefiles:\n",
      "  1) pbs                      5) /agdc_statistics/0.9a7\n",
      "  2) /agdc-py3-env/20171214   6) udunits/2.1.24\n",
      "  3) /agdc-py3/1.5.4          7) /dea-prod/20171219\n",
      "  4) /agdc-py3-prod/1.5.4\n",
      "your output above should look like this:\n",
      "Currently Loaded Modulefiles:\n",
      "  1) /agdc-py3-env/20171214   4) /agdc_statistics/0.9a7\n",
      "  2) /agdc-py3/1.5.4          5) udunits/2.1.24\n",
      "  3) /agdc-py3-prod/1.5.4     6) /dea-prod/20171219\n"
     ]
    }
   ],
   "source": [
    "!module list\n",
    "\n",
    "print(\n",
    "'''your output above should look like this:\n",
    "Currently Loaded Modulefiles:\n",
    "  1) /agdc-py3-env/20171214   4) /agdc_statistics/0.9a7\n",
    "  2) /agdc-py3/1.5.4          5) udunits/2.1.24\n",
    "  3) /agdc-py3-prod/1.5.4     6) /dea-prod/20171219''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-13T03:40:16.537685Z",
     "start_time": "2018-02-13T03:40:16.535248Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### import some modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T05:49:51.911455Z",
     "start_time": "2018-02-26T05:49:51.845153Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import date, timedelta\n",
    "import gdal\n",
    "from gdal import *\n",
    "import numpy\n",
    "\n",
    "import datacube\n",
    "from datacube.helpers import ga_pq_fuser\n",
    "from datacube.storage import masking\n",
    "from datacube.storage.masking import mask_to_dict\n",
    "from datacube_stats.statistics import GeoMedian\n",
    "\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.dates\n",
    "\n",
    "dc = datacube.Datacube(app='dc-tcw and geomedian')\n",
    "\n",
    "#libraries for polygon and polygon mask\n",
    "import fiona\n",
    "import shapely.geometry\n",
    "import rasterio.features\n",
    "import rasterio\n",
    "from datacube.utils import geometry\n",
    "\n",
    "#for drawing rgb composite plots\n",
    "from skimage import exposure\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import imshow\n",
    "\n",
    "#for writing to netcdf\n",
    "from datacube.storage.storage import write_dataset_to_netcdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T03:07:35.632427Z",
     "start_time": "2018-02-14T03:07:35.626091Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Define a function to deal with polygon inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T05:51:45.199521Z",
     "start_time": "2018-02-26T05:51:45.151737Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def open_polygon_from_shapefile(shapefile, index_of_polygon_within_shapefile=0):\n",
    "    '''hide the messy process of getting a polygon input, opening it using fiona and getting the geopolygon out of it for the datacube query \n",
    "    within this function. It will also make sure you have the correct crs object for the DEA'''\n",
    "\n",
    "    # open all the shapes within the shape file\n",
    "    shapes = fiona.open(shapefile)\n",
    "    i =index_of_polygon_within_shapefile\n",
    "    print('shapefile index is '+str(i))\n",
    "    if i > len(shapes):\n",
    "        print('index not in the range for the shapefile'+str(i)+' not in '+str(len(shapes)))\n",
    "        sys.exit(0)\n",
    "    #copy attributes from shapefile and define shape_name\n",
    "    geom_crs = geometry.CRS(shapes.crs_wkt)\n",
    "    geo = shapes[i]['geometry']\n",
    "    geom = geometry.Geometry(geo, crs=geom_crs)\n",
    "    geom_bs = shapely.geometry.shape(shapes[i]['geometry'])\n",
    "    shape_name = shapefile.split('/')[-1].split('.')[0]+'_'+str(i)\n",
    "    print('the name of your shape is '+shape_name)\n",
    "    #get your polygon out as a geom to go into the query, and the shape name for file names later\n",
    "    return geom, shape_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T04:37:51.956824Z",
     "start_time": "2018-02-14T04:37:51.954316Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Define a function to load nbart and pixel quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T05:51:48.813973Z",
     "start_time": "2018-02-26T05:51:48.739869Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_nbart(sensor,query, bands_of_interest): \n",
    "    '''loads nbart data for a sensor, masks using pq, then filters out terrain -999s\n",
    "    function written 23-08-2017 based on dc v1.5.1'''  \n",
    "    dataset = []\n",
    "    product_name = '{}_{}_albers'.format(sensor, 'nbart')\n",
    "    print('loading {}'.format(product_name))\n",
    "    ds = dc.load(product=product_name, measurements=bands_of_interest,\n",
    "                 group_by='solar_day', **query)\n",
    "    #grab crs defs from loaded ds if ds exists\n",
    "    if ds:\n",
    "        crs = ds.crs\n",
    "        affine = ds.affine\n",
    "        print('loaded {}'.format(product_name))\n",
    "        mask_product = '{}_{}_albers'.format(sensor, 'pq')\n",
    "        sensor_pq = dc.load(product=mask_product, fuse_func=ga_pq_fuser,\n",
    "                            group_by='solar_day', **query)\n",
    "        if sensor_pq:\n",
    "            print('making mask {}'.format(mask_product))\n",
    "            cloud_free = masking.make_mask(sensor_pq.pixelquality,\n",
    "                                           cloud_acca='no_cloud',\n",
    "                                           cloud_shadow_acca = 'no_cloud_shadow',                           \n",
    "                                           cloud_shadow_fmask = 'no_cloud_shadow',\n",
    "                                           cloud_fmask='no_cloud',\n",
    "                                           blue_saturated = False,\n",
    "                                           green_saturated = False,\n",
    "                                           red_saturated = False,\n",
    "                                           nir_saturated = False,\n",
    "                                           swir1_saturated = False,\n",
    "                                           swir2_saturated = False,\n",
    "                                           contiguous=True)\n",
    "            ds = ds.where(cloud_free)\n",
    "            ds.attrs['crs'] = crs\n",
    "            ds.attrs['affine'] = affine\n",
    "            print('masked {} with {} and filtered terrain'.format(product_name,mask_product))\n",
    "            # nbarT is correctly used to correct terrain by replacing -999.0 with nan\n",
    "            ds=ds.where(ds!=-999.0)\n",
    "        else: \n",
    "            print('did not mask {} with {}'.format(product_name,mask_product))\n",
    "    else:\n",
    "        print ('did not load {}'.format(product_name)) \n",
    "\n",
    "    if len(ds)>0:\n",
    "        return ds\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T23:02:54.618896Z",
     "start_time": "2018-02-14T23:02:54.616521Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### define a function to draw true color plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T00:09:59.131554Z",
     "start_time": "2018-02-26T00:09:59.105979Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def drawTrueColour(ds, time = 0):\n",
    "    '''code by Mike Barnes Feb 2018 draws true color plots. \n",
    "    altered by bex for drawing composites with no time dimension'''\n",
    "    #t, y, x = ds['red'].shape\n",
    "    y, x = ds['red'].shape\n",
    "    rawimg = np.zeros((y,x,3), dtype = np.float32)\n",
    "    for i, colour in enumerate(['red','green','blue']):\n",
    "        #rawimg[:,:,i] = ds[colour][time].values\n",
    "        rawimg[:,:,i] = ds[colour].values\n",
    "    rawimg[rawimg == -999] = np.nan\n",
    "    img_toshow = exposure.equalize_hist(rawimg, mask = np.isfinite(rawimg))\n",
    "    fig = plt.figure(figsize=[10,10])\n",
    "    imshow(img_toshow)\n",
    "    ax = plt.gca()\n",
    "    #ax.set_title(str(ds.time[time].values))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### define a function to write to netcdf based on the datacube 'write_dataset_to_netcdf' function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T05:54:02.020854Z",
     "start_time": "2018-02-26T05:54:01.993560Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def write_your_netcdf(data, dataset_name, filename, crs):\n",
    "    '''this function turns an xarray dataarray into a dataset so we can write it to netcdf. It adds on a crs definition\n",
    "    from the original array. data = your xarray dataset, dataset_name is a string describing your variable'''    \n",
    "    #turn array into dataset so we can write the netcdf\n",
    "    if isinstance(data,xr.DataArray):\n",
    "        dataset= data.to_dataset(name=dataset_name)\n",
    "    elif isinstance(data,xr.Dataset):\n",
    "        dataset = data\n",
    "    else:\n",
    "        print('your data might be the wrong type, it is: '+type(data))\n",
    "    #grab our crs attributes to write a spatially-referenced netcdf\n",
    "    dataset.attrs['crs'] = crs\n",
    "    #dataset.attrs['affine'] =affine\n",
    "    #dataset.dataset_name.attrs['crs'] = crs\n",
    "    try:\n",
    "        write_dataset_to_netcdf(dataset, filename)\n",
    "    except RuntimeError as err:\n",
    "        print(\"RuntimeError: {0}\".format(err))        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T05:52:34.006168Z",
     "start_time": "2018-02-26T05:52:34.000705Z"
    }
   },
   "source": [
    "### define a function to write to geotiff based on advice from Josh Sixsmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T05:59:02.338113Z",
     "start_time": "2018-02-26T05:59:02.310262Z"
    }
   },
   "outputs": [],
   "source": [
    "def write_your_geotiff(filename, data):\n",
    "    '''this function uses rasterio and numpy to write a multi-band geotiff for one timeslice, or for\n",
    "    a single composite image. It assumes the input data is an xarray dataset (note, dataset not dataarray)\n",
    "    and that you have crs and affine objects attached, and that you are using float data. future users\n",
    "    may wish to assert that these assumptions are correct. Bex Dunn+ Josh Sixsmith 260218'''\n",
    "    \n",
    "    kwargs = {'driver': 'GTiff', 'count': len(data.data_vars),#geomedian no time dim\n",
    "              'width': data.sizes['x'], 'height': data.sizes['y'],\n",
    "              'crs' : data.crs.crs_str,\n",
    "              'transform':data.affine,\n",
    "              'dtype': data.blue.values.dtype,\n",
    "              'nodata': 0,'compress': 'deflate', 'zlevel': 4, 'predictor': 3 }#for ints use 2 for floats use 3}\n",
    "\n",
    "    with rasterio.open(filename, 'w', **kwargs) as src:\n",
    "        for i, band in enumerate(data.data_vars):\n",
    "            src.write(data[band].data, i+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T02:31:06.265607Z",
     "start_time": "2018-02-14T02:31:06.263057Z"
    }
   },
   "source": [
    "### *Change these to specify input and output directories*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T06:00:20.985337Z",
     "start_time": "2018-02-26T06:00:20.978104Z"
    }
   },
   "outputs": [],
   "source": [
    "###input folder is where your data is coming from, so you don't have to type it all the time:\n",
    "input_folder = '/g/data/r78/rjd547/groundwater_activities/Whole_NA/WholeNA_shapes/small_shapes/'\n",
    "\n",
    "###Where you want your data to go when it's saved\n",
    "output_folder = '/g/data/r78/rjd547/groundwater_activities/Arafura_sw/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define input area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T04:34:10.874023Z",
     "start_time": "2018-02-14T04:34:10.871531Z"
    }
   },
   "source": [
    "#### get geometry and name your shape by getting a polygon from a shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T06:00:24.562552Z",
     "start_time": "2018-02-26T06:00:24.545973Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shapefile index is 0\n",
      "the name of your shape is arafura_sml_0\n"
     ]
    }
   ],
   "source": [
    "geom, shape_name = open_polygon_from_shapefile(input_folder+'arafura_sml.shp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T03:05:41.340148Z",
     "start_time": "2018-02-14T03:05:41.335350Z"
    }
   },
   "source": [
    "#### Uncomment for lat/long specs instead of polygon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T06:00:27.381833Z",
     "start_time": "2018-02-26T06:00:27.368185Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Specify lat and long max and minimum corners\n",
    "# lat_min = -20.375 #down\n",
    "# lat_max = -20.340 #up\n",
    "# lon_min = 148.757 #left\n",
    "# lon_max = 148.806 #right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up the datacube query by specifying polygon area, spectral bands, epoch of interest and which landsat sensors you require"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T06:00:28.910712Z",
     "start_time": "2018-02-26T06:00:28.898871Z"
    }
   },
   "outputs": [],
   "source": [
    "#pick a time range\n",
    "start_of_epoch = '2014-01-01'\n",
    "end_of_epoch =  '2017-12-31'\n",
    "\n",
    "#Define wavelengths/bands of interest, remove this kwarg to retrieve all bands\n",
    "bands_of_interest = ['blue',\n",
    "                     'green',\n",
    "                     'red',\n",
    "                     'nir',\n",
    "                     'swir1',\n",
    "                     'swir2'\n",
    "                     ]\n",
    "\n",
    "query = {\n",
    "    'time': (start_of_epoch, end_of_epoch), 'geopolygon': geom,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T06:02:18.569022Z",
     "start_time": "2018-02-26T06:00:31.314409Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading ls8_nbart_albers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:10: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded ls8_nbart_albers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:17: FutureWarning: casting an xarray.Dataset to a boolean will change in xarray v0.11 to only include data variables, not coordinates. Cast the Dataset.variables property instead to preserve existing behavior in a forwards compatible manner.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "making mask ls8_pq_albers\n",
      "masked ls8_nbart_albers with ls8_pq_albers and filtered terrain\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/g/data/v10/public/modules/agdc-py3-env/20171214/envs/agdc/lib/python3.6/site-packages/ipykernel_launcher.py:42: FutureWarning: calling len() on an xarray.Dataset will change in xarray v0.11 to only include data variables, not coordinates. Call len() on the Dataset.variables property instead, like ``len(ds.variables)``, to preserve existing behavior in a forwards compatible manner.\n"
     ]
    }
   ],
   "source": [
    "#this is done separately instead of in a loop because the datasets can be quite large.\n",
    "#currently this is a way of memory handling -there is probably a better way of doing it.\n",
    "#sensor1_nbart=load_nbart('ls5',query, bands_of_interest)\n",
    "#sensor2_nbart=load_nbart('ls7',query,bands_of_interest)\n",
    "sensor3_nbart=load_nbart('ls8',query,bands_of_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### sort the data to make sure the time dimension is sorted properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-26T06:00:35.205Z"
    }
   },
   "outputs": [],
   "source": [
    "nbart=sensor3_nbart.sortby('time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the geomedian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-26T06:00:36.569Z"
    }
   },
   "outputs": [],
   "source": [
    "#geomedian transform\n",
    "nbart_gm=GeoMedian().compute(nbart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-26T06:00:37.161Z"
    }
   },
   "outputs": [],
   "source": [
    "#check that the geomedian is in there\n",
    "print(nbart_gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-14T23:04:02.682868Z",
     "start_time": "2018-02-14T23:04:02.678022Z"
    }
   },
   "source": [
    "### draw a true color plot just to prove we can"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-26T06:00:38.661Z"
    }
   },
   "outputs": [],
   "source": [
    "drawTrueColour(nbart_gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T00:40:00.878144Z",
     "start_time": "2018-02-26T00:40:00.872800Z"
    }
   },
   "source": [
    "### *set up a fancy filename*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-26T06:00:44.735Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = output_folder+shape_name+'_'+start_of_epoch+'_'+end_of_epoch+'_ls8_gmcomposite3'+'.nc'\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write out to netcdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-26T06:00:51.613Z"
    }
   },
   "outputs": [],
   "source": [
    "write_your_netcdf(nbart_gm,'nbart_gm',filename=filename, crs=nbart_gm.crs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-26T06:05:08.067Z"
    }
   },
   "source": [
    "### *set up a fancy filename*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-02-26T06:02:35.866Z"
    }
   },
   "outputs": [],
   "source": [
    "filename = output_folder+shape_name+'_'+start_of_epoch+'_'+end_of_epoch+'_ls8_gmcomposite3'+'.tif'\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Write out to geotiff!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-02-26T05:38:39.699227Z",
     "start_time": "2018-02-26T05:36:59.312Z"
    }
   },
   "outputs": [],
   "source": [
    "write_your_geotiff(filename, nbart_gm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You are done. Now go check your files to make sure everything worked!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
